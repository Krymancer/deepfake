{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import os\n",
    "import sys\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D , MaxPool2D , Flatten , Dropout \n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from keras.models import Model as KerasModel\n",
    "from keras.layers import Input, Dense, Flatten, Conv2D, MaxPooling2D, BatchNormalization, Dropout, Reshape, Concatenate, LeakyReLU\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "np.random.seed(0)\n",
    "tf.compat.v1.random.set_random_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = os.path.join(os.getcwd(), 'data', 'dataset')\n",
    "fake_directory = os.path.join(directory, 'fake')\n",
    "real_directory = os.path.join(directory, 'real')\n",
    "\n",
    "fake = []\n",
    "real = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 199084.11it/s]\n",
      "100%|██████████| 1000/1000 [00:00<00:00, 200052.66it/s]\n"
     ]
    }
   ],
   "source": [
    "for name in tqdm(os.listdir(fake_directory)):\n",
    "  fake.append(os.path.join(fake_directory, name))\n",
    "\n",
    "for name in tqdm(os.listdir(real_directory)):\n",
    "  real.append(os.path.join(real_directory, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400\n"
     ]
    }
   ],
   "source": [
    "y = np.ones(len(real))\n",
    "x = np.zeros(len(fake))\n",
    "\n",
    "label = np.concatenate([y, x])\n",
    "\n",
    "names_list = real + fake\n",
    "\n",
    "new_data, new_label = shuffle(names_list, label,  random_state = 0)\n",
    "\n",
    "size = len(new_data)\n",
    "train_size = int(size * 0.8) * -1\n",
    "test_size = int(size * 0.2) * -1\n",
    "\n",
    "train, test, y_train, y_test = new_data[:test_size], new_data[train_size:], new_label[:train_size], new_label[test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_img(img):\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.uint8)\n",
    "    img = tf.image.resize(img, [224, 224])\n",
    "    return img\n",
    "\n",
    "def get_label(file_path):\n",
    "    cat = tf.strings.split(file_path, ':')[1]\n",
    "    print(cat)\n",
    "    if cat == b'fake':\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def process_path(file_path):\n",
    "    label = get_label(file_path)\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"strided_slice:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "num_threads, num_epochs, train_len = 8, 20, len(train) \n",
    "train_ds = tf.data.Dataset.from_tensor_slices(train)\n",
    "train_ds = train_ds.map(process_path, num_parallel_calls=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = train_ds.shuffle(train_len)\n",
    "train_ds = train_ds.batch(64)\n",
    "train_ds = train_ds.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate= 0.01,\n",
    "    beta_1=0.9,\n",
    "    beta_2=0.999,\n",
    "    epsilon=1e-07,\n",
    "    amsgrad=False)\n",
    "\n",
    "def loss_fn(labels, predictions):\n",
    "    return tf.math.confusion_matrix(\n",
    "    labels, predictions, num_classes=2, weights=None, dtype=tf.dtypes.int32,\n",
    "    name=None).numpy()[0,1]\n",
    "\n",
    "tp = tf.keras.metrics.TruePositives(thresholds=None, name=None, dtype=None)\n",
    "tn = tf.keras.metrics.TrueNegatives(thresholds=None, name=None, dtype=None)\n",
    "fp = tf.keras.metrics.FalsePositives(thresholds=None, name=None, dtype=None)\n",
    "fn = tf.keras.metrics.FalseNegatives(thresholds=None, name=None, dtype=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(model)\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(1024, activation='relu'))\n",
    "classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "classifier.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy', tp,tn,fp,fn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "25/25 [==============================] - 42s 713ms/step - loss: 0.6502 - accuracy: 0.9619 - true_positives_17: 0.0000e+00 - true_negatives_17: 1539.0000 - false_positives_17: 61.0000 - false_negatives_17: 0.0000e+00\n",
      "Epoch 2/20\n",
      "25/25 [==============================] - 20s 717ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 3/20\n",
      "25/25 [==============================] - 20s 720ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 4/20\n",
      "25/25 [==============================] - 20s 720ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 5/20\n",
      "25/25 [==============================] - 20s 721ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 6/20\n",
      "25/25 [==============================] - 20s 722ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 7/20\n",
      "25/25 [==============================] - 20s 720ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 8/20\n",
      "25/25 [==============================] - 20s 723ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 9/20\n",
      "25/25 [==============================] - 20s 722ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 10/20\n",
      "25/25 [==============================] - 20s 721ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 11/20\n",
      "25/25 [==============================] - 20s 724ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 12/20\n",
      "25/25 [==============================] - 20s 721ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 13/20\n",
      "25/25 [==============================] - 20s 721ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 14/20\n",
      "25/25 [==============================] - 20s 723ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 15/20\n",
      "25/25 [==============================] - 20s 723ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 16/20\n",
      "25/25 [==============================] - 20s 727ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 17/20\n",
      "25/25 [==============================] - 20s 725ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 18/20\n",
      "25/25 [==============================] - 20s 727ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 19/20\n",
      "25/25 [==============================] - 20s 726ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n",
      "Epoch 20/20\n",
      "25/25 [==============================] - 20s 727ms/step - loss: 0.0000e+00 - accuracy: 1.0000 - true_positives_17: 0.0000e+00 - true_negatives_17: 1600.0000 - false_positives_17: 0.0000e+00 - false_negatives_17: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "history = classifier.fit(train_ds, epochs = 20, shuffle=True, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>item</th>\n",
       "      <th>max</th>\n",
       "      <th>mean</th>\n",
       "      <th>min</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>loss</td>\n",
       "      <td>0.650192</td>\n",
       "      <td>0.032510</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>accuracy</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.998094</td>\n",
       "      <td>0.961875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true_positives_17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>true_negatives_17</td>\n",
       "      <td>1600.000000</td>\n",
       "      <td>1596.950000</td>\n",
       "      <td>1539.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>false_positives_17</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>3.050000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>false_negatives_17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame()\n",
    "for idx, key in enumerate(history.history.keys()):\n",
    "    row = pd.DataFrame(data={\n",
    "      'item': key,\n",
    "      'max': max(history.history[key]),\n",
    "      'mean': np.mean(history.history[key]),\n",
    "      'min': min(history.history[key]),\n",
    "    }, index=[idx])\n",
    "    df = pd.concat([df,row])\n",
    "\n",
    "from IPython.display import HTML\n",
    "df.sort_values(by=['item'],ascending=True)\n",
    "HTML(df.to_html(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('tf-venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cc9cbefc8544a8379a53c857cd2b15dea463896e3f9720ef4a3e2498fce52563"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
